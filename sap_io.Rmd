---
title: "Wine Dataset - SAP.IO Challenge "
author: "Mudit Goyal"
date: "11/7/2017"
output: html_document
---

# Introduction 

I was given a dataset to work on as part of a coding challenge for 
SAP.iO recruitment! It's a list of ~6000 wines, both white and red. I had
to split it up into two seperate ones because R is not the 
best with memory allocation and my computer simply could not run it.
This problem can be alleviated in the future by putting it in an 
AWS instance and use their cloud computers to be able to better run
these models.

I'm going to work with two models primarily for this - 
- The traditional K-nearest neighbors
- randomForest.
### Pre-process

The first thing I did is go into the CSV file itself is to split the red
and white wines into two different datasets. 

# We will start with the red wine dataset.
```{r}
# The below lines are to set up R so it uses all of my 
# computer's cores in order to run the models much quicker.
library(doParallel)
registerDoParallel(cores = detectCores() - 1)

# Set seed is useful for creating simluations
set.seed(10)

# Loading all the required libraries for my analysis
library(e1071)
library(caret)
library(kknn)
library(randomForest)
library(corrplot)
library(kernlab)
```

```{r}
# Reading the data
df <- read.csv("red.csv")

# Changing NA's to 0's.
df[is.na(df)] <- 0
str(df)
```
Running str(df) displays the internal structure of the red wine dataset. 
It shows that there are 1599 samples and 14 different variables.

We now going to visualize the data using plots for each of the predictor variables. 

```{R}
for (i in c(1:12)) {
    plot(df[, i], jitter(df[, "quality"]), xlab = names(df)[i],
         ylab = "quality", cex = 0.5, cex.lab = 1)
  
    abline(lm(df[, "quality"] ~ df[ ,i]), lty = 3, lwd = 3)
}
```
The line on each of these plots displays the linear regression of our response 
variable __quality__ as a function of each of the predictor variables. When looking 
at each of the plots the first thing that you see are the presence of numerous 
outliers. For example, there's a very glaring outlier in the total sulfur dioxide plot, 
as well as in the density plot. We are going to remove this one from the dataset. 

```{r}
sulfur_max <- which(df$total.sulfur.dioxide == max(df$total.sulfur.dioxide))
df <- df[-sulfur_max, ]
```
We can see that a few of the regression lines show a very weak association to 
our response variable. We'll later split into training and test sets and then we can 
figure out if we want to keep those features or remove them. I created a correlation
plot next to further look at the associations between all the variables.

```{r}
cor_redwines <- cor(df)
# Had some trouble displaying the graph, so going to save as .png and 
# then show in the R markdown file.
png(height = 1200, width = 1500, pointsize = 25, file = 'red_cor_plot.png')
corrplot(cor_redwines, method = 'number')
```
Here's our graph
![](/Users/Mudit/Desktop/SAP.io/red_cor_plot.png)
You can see the weak relationships here between quality, citric.acid, 
free.sulplur dioxide, and also sulphates as shown in the plot. After
processing through the data, we can continue on and say that non-linear 
classification models will be more appropriate than regression, because
of all the weak associations shown in the correlation plot.

### Building the Model

We __need__ to convert our response variable to factor, and then do the 
split into training and testing sets.
```{r}
df$quality <- as.factor(df$quality)
tr <- createDataPartition(df$quality, p = 2/3, list = F)
train_red <- df[tr,]
test_red <- df[-tr,]
```

We are going to go about this using both k-nearest neighors (KNN), along with randomForest.
We will use the caret function which we loaded earlier to tune the model that we can use 
with the train function. We'll repeat 5 times.

### The Caret
I chose to use this library because it really helps to simplify model tuning. 
We can use the tuneGrid argument, which is a grid of all the hyperparameters 
we'd want to use to tune the model which we'll then pass into the train function. 

### Feature Selection
As said above we said that we would decide to use non-linear feature selection 
methods since there are a few factors that have very weak correlations with our 
response variable quality. Most feature selection methods would retain all the 
predictors / excluded 1 at the most - so we are not going to be using feature 
selection while we train and tune our models.

### The Preprocessing
KNN uses distance, so we need to make sure all the predictor variables are standardized. 
We will use the preProcess argument in the train function for this.

### KNN
For KNN, we'll use 5 kmax, 2 distance, and 3 kernel values. 
For the distance, 1 is the Manhattan distance, and 2 is the Euclidian distance.
```{r}

train_ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5)

kknn_grid <- expand.grid(kmax = c(3, 5, 7, 9, 11), distance = c(1, 2),
                         kernel = c("rectangular", "gaussian", "cos"))

kknn_train <- train(quality ~ ., data = train_red, method = "kknn",
                    trControl = train_ctrl, tuneGrid = kknn_grid,
                    preProcess = c("center", "scale"))
plot(kknn_train)
```
```{r}
kknn_train$bestTune
```
The best value for k is 11, after the three repetitions.

### The randomForest model.

For Rf, the only parameter that we can mess around with is ___mtry___, which is 
the number of vars which are randomly sampled at each split.
We'll try values of 1 through 13 to pass through the tuneGrid arguement. 

```{R}
rf_grid <- expand.grid(mtry = 1:13)
rf_train <- train(quality ~ ., data = train_red, method = "rf",
                  trcontrol = train_ctrl, tuneGrid = rf_grid, 
                  preProcess = c("center", "scale"))
plot(rf_train)
```
```{r}
rf_train$bestTune
```
A mtry of 5 is the best value to use here.

### KNN or randomForest?

# Creating the confusion matrix for KNN
```{r}
kknn_predictor <- predict(kknn_train, test_red)
confusionMatrix(kknn_predictor, test_red$quality)
```
# The confusion matrix for our randomForest model.
```{r}
rf_predict <- predict(rf_train, test_red)
confusionMatrix(rf_predict, test_red$quality)
```
For the red wine dataset, the Random Forest Model was the one which
performed the best, with an accuracy of almost 70% with a strong Kappa of 
.5055. The KNN was not better or worse.

==================================================================================================================

# Next, the white wine data set
```{r}
df1 <- read.csv("white.csv")
# changing NA's to 0's.
df1[is.na(df1)] <- 0
str(df1)
```
Running str(df) on the wine dataset shows that there are 4898 samples, and 14 different variables.

Now going to visualize the data using plots for each of the predictor variables. 

```{R}
for (i in c(1:12)) {
    plot(df1[, i], jitter(df1[, "quality"]), xlab = names(df1)[i],
         ylab = "quality", cex = 0.5, cex.lab = 1)
  
    abline(lm(df1[, "quality"] ~ df1[ ,i]), lty = 3, lwd = 3)
}
```
The line on each of these plots displays the linear regression of our response
variable __quality__ as a function of each of the predictor variables. 
When looking at each of the plots the first thing that you see are the presence of 
numerous outliers. For example, there's a very glaring outlier in the 
residual sugar plot, as well as in the density plot. We are going to remove this one from the dataset. 

```{r}
max_sugar <- which(df1$residual.sugar == max(df1$residual.sugar))
df1 <- df1[-max_sugar, ]
```
Again, there are a few regression lines which show a very weak 
association. Like before, we first split into training and test sets and 
then we can figure out if we want to keep those features or remove them. 

```{r}
cor_white <- cor(df1)
png(height = 1200, width = 1500, pointsize = 25, file = 'white_cor_plot.png')
corrplot(cor_white, method = 'number')
```
Here's our graph
![](/Users/Mudit/Desktop/SAP.io/white_cor_plot.png)
You can see the weak relationships here between quality, citric acid, residual sugar, 
free.sulplur dioxide, and also sulphates as shown in the plot. After looking at this data,
after processing through the data, we can continue on and say that non-linear classification
models will be more appropriate than regression.
==================================================================================================================
### Building the Model

We __need__ to convert our response variable to factor, and then do the split 
into training and testing sets.
```{r}
df1$quality <- as.factor(df1$quality)
tr_white <- createDataPartition(df1$quality, p = 2/3, list = F)
train_white <- df1[tr_white,]
test_white <- df1[-tr_white,]
```

We are going to go about this using both k-nearest neighors (KNN), 
along with randomForest. We will use the caret function which we loaded earlier 
to tune the model that we can use with the train function. We'll repeat 3 times.

### The Caret
I chose to use this library because it really helps to simplify model tuning. 
We can use the tuneGrid argument, which is a grid of all the hyperparameters 
we'd want to use to tune the model which we'll then pass into the train function. 

### Feature Selection
As said above we said that we would decide to use non-linear feature selection 
methods since there are a few factors that have very weak correlations with our 
response variable quality. Most feature selection methods would retain all the 
predictors / excluded 1 at the most - so we are not going to be using feature 
selection while we train and tune our models.

### Preprocessing
KNN uses distance, so we need to make sure all the predictor variables are 
standardized. We will use the preProcess argument in the train function for this.

### KNN
For KNN, we'll use 5 kmax, 2 distance, and 3 kernel values. 
For the distance, 1 is the Manhattan distance, and 2 is the Euclidian distance.
```{r}
train_ctrl_white <- trainControl(method = "repeatedcv", number = 5, repeats = 3)

kknn_grid_white <- expand.grid(kmax = c(3, 5, 7, 9, 11), distance = c(1, 2),
                         kernel = c("rectangular", "gaussian", "cos"))

kknn_train_white <- train(quality ~ ., data = train_white, method = "kknn",
                    trControl = train_ctrl_white, tuneGrid = kknn_grid_white,
                    preProcess = c("center", "scale"))
plot(kknn_train_white)
```

```{r}
kknn_train_white$bestTune
```
The best value for k is 11, after the three repetitions.

### The randomForest model.

For this model, it seems that only the mtry 
(number of variables hyperparameter is of use to us. We'll pass 
mtry values of 1-12 into the train function's tuneGrid arg. 
```{R}
rf_grid_white <- expand.grid(mtry = 1:12)
rf_train_white <- train(quality ~ ., data = train_white, method = "rf",
                  trcontrol = train_control_white, tuneGrid = rf_grid_white, 
                  preProcess = c("center", "scale"))
plot(rf_train_white)
```
```{r}
rf_train_white$bestTune
```
A mtry of 2 is the best value to use here.
==================================================================================================================
### The Model Selection
```{r}
kknn_predict_white <- predict(kknn_train_white, test_white)
confusionMatrix(kknn_predict_white, test_white$quality)
```

```{r}
rf_predict_white <- predict(rf_train_white, test_white)
confusionMatrix(rf_predict_white, test_white$quality)
```
For white wine, the random forest model performed better. We have a
95% CI of (.6276, and .6745), and a Kappa level of 0.4494. KNN did 
not perform as well. Both did a rather poor job of identifying white wines
of the 2 lowest and 2 highest classes.
==================================================================================================================
# Finishing up

From our models here, we've learned that it's only accurate to identify
very average quality wines, rendering it not very useful. It is quite difficult 
to conclude that there can be a model that can accurately identify the low and high 
quality wine. 